{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyP+lfavqE8zouX24vXLjftu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuvipaloozie/QM9-GNN/blob/main/QM9_GNN_2D_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPlr7Y0-QiHt"
      },
      "outputs": [],
      "source": [
        "# Installations\n",
        "!pip install torch torch-geometric\n",
        "!pip install rdkit\n",
        "!pip install qm9pack\n",
        "!pip install py3Dmol\n",
        "!pip install pandas numpy matplotlib seaborn tqdm\n",
        "\n",
        "# Core Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Cheminformatics & DL\n",
        "import qm9pack\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem import AllChem\n",
        "import py3Dmol"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic EDA\n",
        "\n",
        "print(\"Loading data from qm9pack... \")\n",
        "df = qm9pack.get_data('qm9')\n",
        "print(f\"Data shape: {df.shape}\\n\")\n",
        "print(\"Data Head:\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\nData Info:\")\n",
        "df.info()\n",
        "\n",
        "TARGET_PROPERTY = 'HOMO_LUMO_gap_au'\n",
        "\n",
        "original_size = len(df)\n",
        "df_clean = df.dropna(subset=['SMILES', TARGET_PROPERTY])\n",
        "cleaned_size = len(df_clean)\n",
        "\n",
        "print(f\"Original number of molecules: {original_size}\")\n",
        "print(f\"Cleaned number of molecules (after dropping NaNs): {cleaned_size}\")\n",
        "\n",
        "for key in df_clean.keys():\n",
        "    qm9pack.helper(key)"
      ],
      "metadata": {
        "id": "9wkD2n0OCZZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizations\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_clean[TARGET_PROPERTY], kde=True, bins=100)\n",
        "plt.title(f'Distribution of {TARGET_PROPERTY}', fontsize=16)\n",
        "plt.xlabel('HOMO-LUMO Gap (au)', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "# Get summary statistics for the target\n",
        "print(df_clean[TARGET_PROPERTY].describe())\n",
        "\n",
        "# Select a subset of key numerical properties\n",
        "numerical_cols = [\n",
        "    'N_atoms', 'Dipole_debye', 'Polarizability_bohr3',\n",
        "    'HOMO_au', 'LUMO_au', 'HOMO_LUMO_gap_au',\n",
        "    'InternalEnergy_0K_au', 'Heatcapacity_Cv_cal_mol_K'\n",
        "]\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_matrix = df_clean[numerical_cols].corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='rocket')\n",
        "plt.title('Correlation Heatmap of Key Properties', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3DMI9_pQC_ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize molecules\n",
        "\n",
        "random_row = df_clean.sample(n=1)\n",
        "smiles_string = random_row['SMILES'].iloc[0]\n",
        "molecule_index = random_row['Index'].iloc[0]\n",
        "gap_value = random_row[TARGET_PROPERTY].iloc[0]\n",
        "\n",
        "print(f\"--- Displaying Random Molecule (Index: {molecule_index}) ---\")\n",
        "print(f\"SMILES: {smiles_string}\")\n",
        "print(f\"HOMO-LUMO Gap: {gap_value:.4f} au\\n\")\n",
        "\n",
        "# 2D Visualization (RDKit)\n",
        "print(\"2D Structure:\")\n",
        "mol_2d = Chem.MolFromSmiles(smiles_string)\n",
        "display(Draw.MolToImage(mol_2d, size=(300, 300)))\n",
        "\n",
        "# 3D Visualization (py3Dmol)\n",
        "print(\"\\n3D Interactive Structure:\")\n",
        "mol_3d = Chem.MolFromSmiles(smiles_string)\n",
        "mol_3d = Chem.AddHs(mol_3d)\n",
        "AllChem.EmbedMolecule(mol_3d, AllChem.ETKDG())\n",
        "AllChem.MMFFOptimizeMolecule(mol_3d)\n",
        "mblock = Chem.MolToMolBlock(mol_3d)\n",
        "\n",
        "view = py3Dmol.view(width=500, height=400)\n",
        "view.addModel(mblock, 'mol')\n",
        "view.setStyle({'stick':{}, 'sphere': {'scale':0.3}})\n",
        "view.zoomTo()\n",
        "view.show()"
      ],
      "metadata": {
        "id": "SP21UL2XDLtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.utils import from_smiles\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\"Converting SMILES to graphs for GCN model...\")\n",
        "data_list = []\n",
        "for index, row in tqdm(df_clean.iterrows(), total=df_clean.shape[0]):\n",
        "    try:\n",
        "        data = from_smiles(row['SMILES'])\n",
        "        data.y = torch.tensor([[row[TARGET_PROPERTY]]], dtype=torch.float)\n",
        "        data_list.append(data)\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "print(f\"Successfully converted {len(data_list)} molecules.\")\n",
        "\n",
        "# Create Train/Validation/Test Splits and DataLoaders\n",
        "torch.manual_seed(42)\n",
        "data_list = sorted(data_list, key=lambda x: torch.rand(1)) # Shuffle\n",
        "train_size = int(0.8 * len(data_list))\n",
        "val_size = int(0.1 * len(data_list))\n",
        "test_size = len(data_list) - train_size - val_size\n",
        "\n",
        "train_data = data_list[:train_size]\n",
        "val_data = data_list[train_size:train_size + val_size]\n",
        "test_data = data_list[train_size + val_size:]\n",
        "\n",
        "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
        "\n",
        "# Use a standard batch size for this scalar regression task\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=64)\n",
        "test_loader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class BaselineGCN(torch.nn.Module):\n",
        "    def __init__(self, feature_size, model_dim):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(42)\n",
        "        # GCNConv is the classic, simpler graph convolution\n",
        "        self.conv1 = GCNConv(feature_size, model_dim)\n",
        "        self.conv2 = GCNConv(model_dim, model_dim)\n",
        "        self.conv3 = GCNConv(model_dim, model_dim)\n",
        "\n",
        "        # Readout layers\n",
        "        self.linear1 = Linear(model_dim, 128)\n",
        "        self.linear2 = Linear(128, 1) # Output is 1 value\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "\n",
        "        x = x.float()\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        x = F.elu(self.conv2(x, edge_index))\n",
        "        x = F.elu(self.conv3(x, edge_index))\n",
        "\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = F.elu(self.linear1(x))\n",
        "        x = self.linear2(x) # Final prediction\n",
        "        return x\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"--- Using device: {device} ---\")\n",
        "\n",
        "model_dim = 64\n",
        "feature_size = data_list[0].num_features\n",
        "\n",
        "gcn_model = BaselineGCN(feature_size, model_dim).to(device)\n",
        "optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.001)\n",
        "# We use L1Loss (Mean Absolute Error) as it's robust to outliers\n",
        "loss_fn = torch.nn.L1Loss()\n",
        "\n",
        "def train_gcn(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = loss_fn(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_gcn(model, loader):\n",
        "    model.eval()\n",
        "    total_mae = 0\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            mae = loss_fn(out, data.y)\n",
        "            total_mae += mae.item() * data.num_graphs\n",
        "    return total_mae / len(loader.dataset)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "\n",
        "\n",
        "print(\"--- Starting GCN Model Training (Tracking Loss) ---\")\n",
        "num_epochs = 25\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_gcn(gcn_model, train_loader)\n",
        "    val_mae = eval_gcn(gcn_model, val_loader)\n",
        "\n",
        "    train_loss_history.append(train_loss)\n",
        "    val_loss_history.append(val_mae)\n",
        "\n",
        "    print(f'Epoch: {epoch:02d}, Train Loss (MAE): {train_loss:.4f}, Val MAE: {val_mae:.4f}')\n",
        "\n",
        "print(\"\\n--- GCN Model Training Complete ---\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, num_epochs + 1), train_loss_history, label='Training Loss (MAE)')\n",
        "plt.plot(range(1, num_epochs + 1), val_loss_history, label='Validation Loss (MAE)')\n",
        "plt.title('GCN: Training vs. Validation Loss', fontsize=16)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss (Mean Absolute Error)', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "test_mae_gcn = eval_gcn(gcn_model, test_loader)\n",
        "print(f'Final Test MAE for GCN Model: {test_mae_gcn:.4f} au')"
      ],
      "metadata": {
        "id": "8XP4Cj_LE3RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def get_predictions(model, loader):\n",
        "    \"\"\"Utility function to get predictions and labels from the test set.\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(loader, desc=\"Getting Predictions\", leave=False):\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "\n",
        "            all_preds.append(out.cpu().numpy())\n",
        "            all_labels.append(data.y.cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds).flatten()\n",
        "    all_labels = np.concatenate(all_labels).flatten()\n",
        "    return all_preds, all_labels\n",
        "\n",
        "print(\"Gathering predictions from the GCN model...\")\n",
        "y_pred_gcn, y_true = get_predictions(gcn_model, test_loader)\n",
        "\n",
        "print(\"Predictions and labels gathered. Ready for plotting.\")\n",
        "\n",
        "r2_gcn = r2_score(y_true, y_pred_gcn)\n",
        "print(f'GCN Model R-squared (R²): {r2_gcn:.4f}')\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "sns.scatterplot(x=y_true, y=y_pred_gcn, alpha=0.3, s=20, label='Predictions')\n",
        "min_val = min(y_true.min(), y_pred_gcn.min())\n",
        "max_val = max(y_true.max(), y_pred_gcn.max())\n",
        "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', lw=2, label='Parity Line')\n",
        "\n",
        "plt.text(min_val + (max_val - min_val) * 0.05,  # 5% from the left\n",
        "         max_val - (max_val - min_val) * 0.05,  # 5% from the top\n",
        "         f'$R^2 = {r2_gcn:.4f}$',\n",
        "         fontsize=14,\n",
        "         va='top', # Vertical alignment\n",
        "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.5)) # Add a semi-transparent box\n",
        "\n",
        "plt.title('GCN: Predicted vs. Actual (with $R^2$ Score)', fontsize=16)\n",
        "plt.xlabel('Actual HOMO-LUMO Gap (au)', fontsize=12)\n",
        "plt.ylabel('Predicted HOMO-LUMO Gap (au)', fontsize=12)\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "residuals_gcn = y_true - y_pred_gcn\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "\n",
        "sns.scatterplot(x=y_pred_gcn, y=residuals_gcn, alpha=0.3, s=20)\n",
        "\n",
        "plt.axhline(y=0, color='red', linestyle='--', lw=2)\n",
        "\n",
        "plt.title('GCN: Residual Plot', fontsize=16)\n",
        "plt.xlabel('Predicted HOMO-LUMO Gap (au)', fontsize=12)\n",
        "plt.ylabel('Residual (Actual - Predicted)', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "See1WEvSL_XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import GATv2Conv, global_mean_pool\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class BaselineGAT(torch.nn.Module):\n",
        "    def __init__(self, feature_size, model_dim):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(42)\n",
        "        # GATv2 with 4 attention heads\n",
        "        self.conv1 = GATv2Conv(feature_size, model_dim, heads=4)\n",
        "        self.conv2 = GATv2Conv(model_dim * 4, model_dim, heads=4)\n",
        "        self.conv3 = GATv2Conv(model_dim * 4, model_dim, heads=4)\n",
        "\n",
        "        # Readout layers\n",
        "        self.linear1 = Linear(model_dim * 4, 128)\n",
        "        self.linear2 = Linear(128, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        x = x.float()\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        x = F.elu(self.conv2(x, edge_index))\n",
        "        x = F.elu(self.conv3(x, edge_index))\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = F.elu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model_dim = 64\n",
        "feature_size = data_list[0].num_features\n",
        "\n",
        "gat_model = BaselineGAT(feature_size, model_dim).to(device)\n",
        "optimizer_gat = torch.optim.Adam(gat_model.parameters(), lr=0.001)\n",
        "loss_fn_gat = torch.nn.L1Loss()\n",
        "\n",
        "def train_gat(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = loss_fn(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def eval_gat(model, loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_mae = 0\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            mae = loss_fn(out, data.y)\n",
        "            total_mae += mae.item() * data.num_graphs\n",
        "    return total_mae / len(loader.dataset)\n",
        "\n",
        "\n",
        "print(\"--- Starting GATv2 Model Training (Advanced Baseline) ---\")\n",
        "num_epochs = 25\n",
        "\n",
        "# Lists to store loss history for plotting\n",
        "gat_train_loss_history = []\n",
        "gat_val_loss_history = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_gat(gat_model, train_loader, optimizer_gat, loss_fn_gat)\n",
        "    val_mae = eval_gat(gat_model, val_loader, loss_fn_gat)\n",
        "\n",
        "    # Save history\n",
        "    gat_train_loss_history.append(train_loss)\n",
        "    gat_val_loss_history.append(val_mae)\n",
        "\n",
        "    print(f'Epoch: {epoch:02d}, Train Loss (MAE): {train_loss:.4f}, Val MAE: {val_mae:.4f}')\n",
        "\n",
        "print(\"\\n--- GATv2 Model Training Complete ---\")\n",
        "test_mae_gat = eval_gat(gat_model, test_loader, loss_fn_gat)\n",
        "print(f'Final Test MAE for GATv2 Model: {test_mae_gat:.4f} au')\n",
        "try:\n",
        "    print(f'Final Test MAE for GCN Model:  {test_mae_gcn:.4f} au')\n",
        "except NameError:\n",
        "    print(\"Run GCN model cell to see GCN MAE.\")"
      ],
      "metadata": {
        "id": "BU-ZB9sGQ5d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, num_epochs + 1), gat_train_loss_history, label='Training Loss (MAE)')\n",
        "plt.plot(range(1, num_epochs + 1), gat_val_loss_history, label='Validation Loss (MAE)')\n",
        "plt.title('GATv2: Training vs. Validation Loss', fontsize=16)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss (Mean Absolute Error)', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MaMxUUASQ7pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def get_predictions(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(loader, desc=\"Getting Predictions\", leave=False):\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            all_preds.append(out.cpu().numpy())\n",
        "            all_labels.append(data.y.cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds).flatten()\n",
        "    all_labels = np.concatenate(all_labels).flatten()\n",
        "    return all_preds, all_labels\n",
        "\n",
        "\n",
        "print(\"Gathering predictions from the GATv2 model...\")\n",
        "y_pred_gat, y_true_gat = get_predictions(gat_model, test_loader)\n",
        "\n",
        "\n",
        "r2_gat = r2_score(y_true_gat, y_pred_gat)\n",
        "print(f'GATv2 Model R-squared (R²): {r2_gat:.4f}')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.scatterplot(x=y_true_gat, y=y_pred_gat, alpha=0.3, s=20, label='GAT Predictions')\n",
        "\n",
        "min_val = min(y_true_gat.min(), y_pred_gat.min())\n",
        "max_val = max(y_true_gat.max(), y_pred_gat.max())\n",
        "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', lw=2, label='Parity Line')\n",
        "\n",
        "plt.text(min_val + (max_val - min_val) * 0.05,\n",
        "         max_val - (max_val - min_val) * 0.05,\n",
        "         f'$R^2 = {r2_gat:.4f}$',\n",
        "         fontsize=14, va='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
        "\n",
        "plt.title('GATv2: Predicted vs. Actual (with $R^2$ Score)', fontsize=16)\n",
        "plt.xlabel('Actual HOMO-LUMO Gap (au)', fontsize=12)\n",
        "plt.ylabel('Predicted HOMO-LUMO Gap (au)', fontsize=12)\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "residuals_gat = y_true_gat - y_pred_gat\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.scatterplot(x=y_pred_gat, y=residuals_gat, alpha=0.3, s=20)\n",
        "plt.axhline(y=0, color='red', linestyle='--', lw=2)\n",
        "plt.title('GATv2: Residual Plot', fontsize=16)\n",
        "plt.xlabel('Predicted HOMO-LUMO Gap (au)', fontsize=12)\n",
        "plt.ylabel('Residual (Actual - Predicted)', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lI20kxEhQ-Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-create data_list, this time adding the .smiles attribute\n",
        "print(\"Re-processing data to include SMILES strings for analysis...\")\n",
        "\n",
        "data_list_with_smiles = []\n",
        "for index, row in tqdm(df_clean.iterrows(), total=df_clean.shape[0]):\n",
        "    try:\n",
        "        data = from_smiles(row['SMILES'])\n",
        "        data.y = torch.tensor([[row[TARGET_PROPERTY]]], dtype=torch.float)\n",
        "        data.smiles = row['SMILES']\n",
        "        data_list_with_smiles.append(data)\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "# We use the same seed to ensure the shuffle is identical to before\n",
        "torch.manual_seed(42)\n",
        "data_list_shuffled = sorted(data_list_with_smiles, key=lambda x: torch.rand(1))\n",
        "\n",
        "train_size = int(0.8 * len(data_list_shuffled))\n",
        "val_size = int(0.1 * len(data_list_shuffled))\n",
        "test_size = len(data_list_shuffled) - train_size - val_size\n",
        "\n",
        "analysis_test_data = data_list_shuffled[train_size + val_size:]\n",
        "\n",
        "analysis_test_loader = DataLoader(analysis_test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "print(f\"Created new test loader with {len(analysis_test_data)} molecules.\")"
      ],
      "metadata": {
        "id": "oTSaF02VUKNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import py3Dmol\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "\n",
        "def get_predictions_with_details(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_smiles = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(loader, desc=\"Getting Predictions\", leave=False):\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "\n",
        "            all_preds.append(out.cpu().numpy())\n",
        "            all_labels.append(data.y.cpu().numpy())\n",
        "            # We can access the .smiles attribute from the batch\n",
        "            all_smiles.extend(data.smiles)\n",
        "\n",
        "    all_preds = np.concatenate(all_preds).flatten()\n",
        "    all_labels = np.concatenate(all_labels).flatten()\n",
        "    return all_preds, all_labels, all_smiles\n",
        "\n",
        "\n",
        "y_pred, y_true, smiles = get_predictions_with_details(gat_model, analysis_test_loader)\n",
        "\n",
        "\n",
        "error_df = pd.DataFrame({\n",
        "    'SMILES': smiles,\n",
        "    'Actual_Gap': y_true,\n",
        "    'Predicted_Gap': y_pred\n",
        "})\n",
        "error_df['Absolute_Error'] = (error_df['Actual_Gap'] - error_df['Predicted_Gap']).abs()\n",
        "\n",
        "\n",
        "error_df_sorted = error_df.sort_values(by='Absolute_Error', ascending=False)\n",
        "\n",
        "print(\"--- GATv2 Model Failure Analysis ---\")\n",
        "display(error_df_sorted.head(10))\n",
        "\n",
        "print(\"--- Visualizing Top 10 Worst Predictions ---\")\n",
        "for index, row in error_df_sorted.head(10).iterrows():\n",
        "    print(f\"--- Molecule Rank #{index + 1} ---\")\n",
        "    print(f\"SMILES: {row['SMILES']}\")\n",
        "    print(f\"Actual Gap: {row['Actual_Gap']:.4f} au\")\n",
        "    print(f\"Predicted Gap: {row['Predicted_Gap']:.4f} au\")\n",
        "    print(f\"Absolute Error: {row['Absolute_Error']:.4f} au\")\n",
        "\n",
        "    # Generate 3D model\n",
        "    mol = Chem.MolFromSmiles(row['SMILES'])\n",
        "    mol = Chem.AddHs(mol)\n",
        "\n",
        "\n",
        "    embed_code = AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
        "\n",
        "    if embed_code == 0:\n",
        "        try:\n",
        "\n",
        "            AllChem.MMFFOptimizeMolecule(mol)\n",
        "\n",
        "            mblock = Chem.MolToMolBlock(mol)\n",
        "\n",
        "            view = py3Dmol.view(width=500, height=400)\n",
        "            view.addModel(mblock, 'mol')\n",
        "            view.setStyle({'stick':{}, 'sphere': {'scale':0.3}})\n",
        "            view.zoomTo()\n",
        "            view.show()\n",
        "\n",
        "        except ValueError as e:\n",
        "            print(f\"  -> WARNING: MMFFOptimize failed: {e}\")\n",
        "            print(\"  -> Skipping 3D visualization for this molecule.\")\n",
        "    else:\n",
        "        # If embed_code was -1 (failure)\n",
        "        print(f\"  -> WARNING: RDKit's EmbedMolecule failed to generate 3D conformer.\")\n",
        "        print(\"  -> Skipping 3D visualization for this molecule.\")\n",
        "\n",
        "    print(\"-\" * 20) # Separator"
      ],
      "metadata": {
        "id": "cAyVbNfWUMWb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}